#!/usr/bin/env python
"""
Script to fetch new URLs from Ralph Lauren sitemaps.

This script downloads the sitemap index from Ralph Lauren and enumerates
all sitemap files listed therein. Each sitemap is fetched and parsed
for URLs. A persistent text file, ``all_urls.txt``, is used to track
previously seen URLs. Any URLs that are present in the current run but
were not seen before are considered "new" and are written to a file
named ``new_urls_YYYY-MM-DD.txt`` based on the current UTC date.

The script is intended to be executed on a schedule (e.g. via GitHub
Actions) and will gracefully handle cases where there are no new URLs
or when network errors occur. It avoids counting updated URLs as new
by only considering whether the URL itself has been seen before.

Usage:
    python fetch_new_urls.py

Dependencies:
    - requests
    - Python 3.8+

Author: Auto-generated by AI
"""

import datetime
import os
import sys
from typing import Iterable, List, Set

import requests
from xml.etree import ElementTree as ET


def fetch_xml(url: str) -> ET.Element:
    """Fetch an XML document from a URL and return its root element.

    Args:
        url: The URL to fetch.

    Returns:
        An ElementTree Element representing the root of the XML document.

    Raises:
        requests.HTTPError: If the HTTP request fails.
        ET.ParseError: If the response cannot be parsed as XML.
    """
    response = requests.get(url, timeout=30)
    response.raise_for_status()
    return ET.fromstring(response.content)


def extract_sitemap_urls(index_root: ET.Element) -> List[str]:
    """Extract sitemap locations from a sitemap index XML root.

    Args:
        index_root: The root element of a sitemap index XML document.

    Returns:
        A list of sitemap URLs found in the index.
    """
    # Namespaces may be present; we ignore them by using wildcard
    return [loc.text.strip() for loc in index_root.findall(".//{*}loc") if loc.text]


def extract_urls(urlset_root: ET.Element) -> List[str]:
    """Extract URL locations from a sitemap URL set XML root.

    Args:
        urlset_root: The root element of a sitemap (urlset) XML document.

    Returns:
        A list of URL strings.
    """
    return [loc.text.strip() for loc in urlset_root.findall(".//{*}loc") if loc.text]


def get_current_urls(sitemap_urls: Iterable[str]) -> List[str]:
    """Fetch and combine URLs from a list of sitemap URLs.

    Args:
        sitemap_urls: An iterable of sitemap file URLs.

    Returns:
        A sorted list of unique URLs currently present across all provided sitemaps.
    """
    all_urls: Set[str] = set()
    for sm_url in sitemap_urls:
        try:
            root = fetch_xml(sm_url)
        except Exception as exc:
            print(f"Warning: Failed to fetch or parse sitemap '{sm_url}': {exc}", file=sys.stderr)
            continue
        urls = extract_urls(root)
        for u in urls:
            all_urls.add(u)
    return sorted(all_urls)


def load_previous_urls(path: str) -> Set[str]:
    """Load previously seen URLs from a file.

    Args:
        path: Path to the file containing URLs, one per line.

    Returns:
        A set of URLs. Returns an empty set if the file does not exist.
    """
    if not os.path.exists(path):
        return set()
    with open(path, "r", encoding="utf-8") as f:
        return {line.strip() for line in f if line.strip()}


def save_urls(path: str, urls: Iterable[str]) -> None:
    """Save URLs to a file, one per line.

    Args:
        path: Path to the file to write.
        urls: An iterable of URL strings.
    """
    with open(path, "w", encoding="utf-8") as f:
        for url in urls:
            f.write(url + "\n")


def main() -> int:
    """Entry point for the script.

    Returns:
        Zero on success, non-zero on failure.
    """
    # URL of the sitemap index. Use this to discover individual sitemap files.
    # Ralph Lauren exposes its primary sitemap index at `/index` rather than
    # `/sitemap.xml`【336331664324739†L68-L87】. If this URL is unreachable, the
    # fallback list below is used.
    sitemap_index_url = "https://www.ralphlauren.com/index"
    # Fallback list of sitemaps if the index cannot be fetched or parsed.
    fallback_sitemaps = [
        "https://www.ralphlauren.com/seocontent?n=contentsitemap_0",
        "https://www.ralphlauren.com/seocontent?n=productsitemap_0",
        "https://www.ralphlauren.com/seocontent?n=categorysitemap",
        "https://www.ralphlauren.com/seocontent?n=StoreSiteMap",
        "https://www.ralphlauren.com/seocontent?n=facetsitemap",
    ]
    sitemap_urls: List[str]
    try:
        # Attempt to fetch and parse the sitemap index
        root = fetch_xml(sitemap_index_url)
        # Only treat it as an index if the root tag ends with 'sitemapindex'
        if root.tag.endswith("sitemapindex"):
            sitemap_urls = extract_sitemap_urls(root)
        else:
            print(f"Note: Sitemap index root tag '{root.tag}' is not 'sitemapindex'. Using fallback list.")
            sitemap_urls = fallback_sitemaps
    except Exception as exc:
        print(f"Warning: Failed to retrieve or parse sitemap index '{sitemap_index_url}': {exc}", file=sys.stderr)
        sitemap_urls = fallback_sitemaps

    if not sitemap_urls:
        print("Error: No sitemaps found to process.", file=sys.stderr)
        return 1

    print(f"Processing {len(sitemap_urls)} sitemap(s) ...")
    current_urls = get_current_urls(sitemap_urls)
    print(f"Retrieved {len(current_urls)} unique URLs in total.")

    # Load previously seen URLs from file
    archive_file = "all_urls.txt"
    previous_urls = load_previous_urls(archive_file)
    # Compute which URLs are new compared to those previously seen. We track
    # the set of previously seen URLs before updating it so that we can
    # report how many URLs were already known (pre-existing) at the time of
    # this run. This allows the front‑end to distinguish between the number
    # of URLs that were already known and the total number after the run.
    new_urls_set = set(current_urls) - previous_urls
    new_urls = sorted(new_urls_set)

    date_str = datetime.datetime.utcnow().strftime("%Y-%m-%d")
    output_filename = f"new_urls_{date_str}.txt"
    if new_urls:
        # Write the per‑date file containing only the new URLs discovered today
        save_urls(output_filename, new_urls)
        print(f"Found {len(new_urls)} new URL(s). Saved to '{output_filename}'.")
    else:
        # If there are no new URLs, ensure the dated file exists as an empty file
        # so the front‑end can indicate that no new URLs were added.
        if not os.path.exists(output_filename):
            open(output_filename, "w").close()
        print("No new URLs detected.")
    # Always update the latest_new_urls.txt file with the most recent new URLs
    # This file is used by the static front‑end to display daily additions.
    latest_file = "latest_new_urls.txt"
    if new_urls:
        save_urls(latest_file, new_urls)
    else:
        # Write an empty file if there are no new URLs today
        open(latest_file, "w").close()

    # Update the archive file if changes are detected
    if new_urls_set or (set(current_urls) != previous_urls):
        save_urls(archive_file, current_urls)
        print(f"Updated archive file '{archive_file}'.")
    else:
        print(f"No updates to '{archive_file}'.")

    # ----------------------------------------------------------------------
    # Persist run metadata for reporting
    #
    # To provide a simple log of when the script last ran and how many URLs
    # were discovered, we maintain a JSON file (run_log.json). Each entry
    # records the date of the run, a precise timestamp (UTC), the count of
    # new URLs discovered, and the total number of unique URLs seen across
    # all sitemaps for that run. When the script runs again on the same date,
    # the existing entry for that date is replaced. Otherwise, a new entry
    # is appended to the log. The resulting JSON array is written back
    # with indentation for readability.
    import json
    log_filename = "run_log.json"
    # Determine how many URLs were already known before this run. This
    # represents the pre‑existing URLs in the archive at the moment of
    # execution. The number of new URLs plus the number of existing URLs
    # should equal the total number of URLs discovered in this run.
    existing_urls_count = len(previous_urls)
    run_entry = {
        "date": date_str,
        # Use ISO 8601 format with 'Z' to indicate UTC time
        "timestamp": datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
        "new_urls_count": len(new_urls),
        "existing_urls_count": existing_urls_count,
        "total_urls_count": len(current_urls),
    }
    run_history = []
    if os.path.exists(log_filename):
        try:
            with open(log_filename, "r", encoding="utf-8") as jf:
                data = json.load(jf)
            # Ensure we have a list
            if isinstance(data, list):
                run_history = data
        except Exception as exc:
            print(f"Warning: Failed to read existing run log: {exc}", file=sys.stderr)
            run_history = []
    # Remove any existing entry for the same date
    run_history = [entry for entry in run_history if entry.get("date") != date_str]
    # Append the new entry
    run_history.append(run_entry)
    # Sort by date descending (most recent first)
    try:
        run_history.sort(key=lambda e: e.get("date"), reverse=True)
    except Exception:
        pass
    # Write back to file
    try:
        with open(log_filename, "w", encoding="utf-8") as jf:
            json.dump(run_history, jf, indent=2)
    except Exception as exc:
        print(f"Warning: Failed to write run log: {exc}", file=sys.stderr)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
