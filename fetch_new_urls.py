#!/usr/bin/env python
"""
Script to fetch new URLs from Ralph Lauren sitemaps.

This script downloads the sitemap index from Ralph Lauren and enumerates
all sitemap files listed therein. Each sitemap is fetched and parsed
for URLs. A persistent text file, ``all_urls.txt``, is used to track
previously seen URLs. Any URLs that are present in the current run but
were not seen before are considered "new" and are written to a file
named ``new_urls_YYYY-MM-DD.txt`` based on the current UTC date.

The script is intended to be executed on a schedule (e.g. via GitHub
Actions) and will gracefully handle cases where there are no new URLs
or when network errors occur. It avoids counting updated URLs as new
by only considering whether the URL itself has been seen before.

Usage:
    python fetch_new_urls.py

Dependencies:
    - requests
    - Python 3.8+

Author: Auto-generated by AI
"""

import datetime
import os
import sys
from typing import Iterable, List, Set

import requests
from xml.etree import ElementTree as ET


def fetch_xml(url: str) -> ET.Element:
    """Fetch an XML document from a URL and return its root element.

    Args:
        url: The URL to fetch.

    Returns:
        An ElementTree Element representing the root of the XML document.

    Raises:
        requests.HTTPError: If the HTTP request fails.
        ET.ParseError: If the response cannot be parsed as XML.
    """
    response = requests.get(url, timeout=30)
    response.raise_for_status()
    return ET.fromstring(response.content)


def extract_sitemap_urls(index_root: ET.Element) -> List[str]:
    """Extract sitemap locations from a sitemap index XML root.

    Args:
        index_root: The root element of a sitemap index XML document.

    Returns:
        A list of sitemap URLs found in the index.
    """
    # Namespaces may be present; we ignore them by using wildcard
    return [loc.text.strip() for loc in index_root.findall(".//{*}loc") if loc.text]


def extract_urls(urlset_root: ET.Element) -> List[str]:
    """Extract URL locations from a sitemap URL set XML root.

    Args:
        urlset_root: The root element of a sitemap (urlset) XML document.

    Returns:
        A list of URL strings.
    """
    return [loc.text.strip() for loc in urlset_root.findall(".//{*}loc") if loc.text]


def get_current_urls(sitemap_urls: Iterable[str]) -> List[str]:
    """Fetch and combine URLs from a list of sitemap URLs.

    Args:
        sitemap_urls: An iterable of sitemap file URLs.

    Returns:
        A sorted list of unique URLs currently present across all provided sitemaps.
    """
    all_urls: Set[str] = set()
    for sm_url in sitemap_urls:
        try:
            root = fetch_xml(sm_url)
        except Exception as exc:
            print(f"Warning: Failed to fetch or parse sitemap '{sm_url}': {exc}", file=sys.stderr)
            continue
        urls = extract_urls(root)
        for u in urls:
            all_urls.add(u)
    return sorted(all_urls)


def load_previous_urls(path: str) -> Set[str]:
    """Load previously seen URLs from a file.

    Args:
        path: Path to the file containing URLs, one per line.

    Returns:
        A set of URLs. Returns an empty set if the file does not exist.
    """
    if not os.path.exists(path):
        return set()
    with open(path, "r", encoding="utf-8") as f:
        return {line.strip() for line in f if line.strip()}


def save_urls(path: str, urls: Iterable[str]) -> None:
    """Save URLs to a file, one per line.

    Args:
        path: Path to the file to write.
        urls: An iterable of URL strings.
    """
    with open(path, "w", encoding="utf-8") as f:
        for url in urls:
            f.write(url + "\n")


def main() -> int:
    """Entry point for the script.

    Returns:
        Zero on success, non-zero on failure.
    """
    # URL of the sitemap index. Use this to discover individual sitemap files.
    # Ralph Lauren exposes its primary sitemap index at `/index` rather than
    # `/sitemap.xml`【336331664324739†L68-L87】. If this URL is unreachable, the
    # fallback list below is used.
    sitemap_index_url = "https://www.ralphlauren.com/index"
    # Fallback list of sitemaps if the index cannot be fetched or parsed.
    fallback_sitemaps = [
        "https://www.ralphlauren.com/seocontent?n=contentsitemap_0",
        "https://www.ralphlauren.com/seocontent?n=productsitemap_0",
        "https://www.ralphlauren.com/seocontent?n=categorysitemap",
        "https://www.ralphlauren.com/seocontent?n=StoreSiteMap",
        "https://www.ralphlauren.com/seocontent?n=facetsitemap",
    ]
    sitemap_urls: List[str]
    try:
        # Attempt to fetch and parse the sitemap index
        root = fetch_xml(sitemap_index_url)
        # Only treat it as an index if the root tag ends with 'sitemapindex'
        if root.tag.endswith("sitemapindex"):
            sitemap_urls = extract_sitemap_urls(root)
        else:
            print(f"Note: Sitemap index root tag '{root.tag}' is not 'sitemapindex'. Using fallback list.")
            sitemap_urls = fallback_sitemaps
    except Exception as exc:
        print(f"Warning: Failed to retrieve or parse sitemap index '{sitemap_index_url}': {exc}", file=sys.stderr)
        sitemap_urls = fallback_sitemaps

    if not sitemap_urls:
        print("Error: No sitemaps found to process.", file=sys.stderr)
        return 1

    print(f"Processing {len(sitemap_urls)} sitemap(s) ...")
    current_urls = get_current_urls(sitemap_urls)
    print(f"Retrieved {len(current_urls)} unique URLs in total.")

    # Load previously seen URLs from file
    archive_file = "all_urls.txt"
    previous_urls = load_previous_urls(archive_file)
    new_urls_set = set(current_urls) - previous_urls
    new_urls = sorted(new_urls_set)

    date_str = datetime.datetime.utcnow().strftime("%Y-%m-%d")
    output_filename = f"new_urls_{date_str}.txt"
    if new_urls:
        # Write the per‑date file containing only the new URLs discovered today
        save_urls(output_filename, new_urls)
        print(f"Found {len(new_urls)} new URL(s). Saved to '{output_filename}'.")
    else:
        # If there are no new URLs, ensure the dated file exists as an empty file
        # so the front‑end can indicate that no new URLs were added.
        if not os.path.exists(output_filename):
            open(output_filename, "w").close()
        print("No new URLs detected.")
    # Always update the latest_new_urls.txt file with the most recent new URLs
    # This file is used by the static front‑end to display daily additions.
    latest_file = "latest_new_urls.txt"
    if new_urls:
        save_urls(latest_file, new_urls)
    else:
        # Write an empty file if there are no new URLs today
        open(latest_file, "w").close()

    # Update the archive file if changes are detected
    if new_urls_set or (set(current_urls) != previous_urls):
        save_urls(archive_file, current_urls)
        print(f"Updated archive file '{archive_file}'.")
    else:
        print(f"No updates to '{archive_file}'.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
